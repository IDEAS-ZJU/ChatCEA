{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e780d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for various functionalities\n",
    "import os\n",
    "# Set your OpenAI API key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Add your API KEY for OPENAI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries for subprocess, concurrent tasks, and PDF processing\n",
    "import subprocess\n",
    "import uuid\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from tqdm import tqdm  # For progress bars\n",
    "from PyPDF2 import PdfReader  # For handling PDF files\n",
    "\n",
    "# Importing LangChain specific components for document processing and retrieval\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever, SearchType\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from BCEmbedding import RerankerModel\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def june_run_nougat(file_path, output_dir):\n",
    "    \"\"\"\n",
    "    Run Nougat tool on the given file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input PDF file.\n",
    "        output_dir (str): Directory to store the output.\n",
    "\n",
    "    Returns:\n",
    "        int: 0 if operation is successful, 1 if failed.\n",
    "    \"\"\"\n",
    "    cmd = [\"nougat.exe\", file_path, \"-o\", output_dir, \"-m\", \"0.1.0-base\", \"--no-skipping\"]\n",
    "    res = subprocess.run(cmd)\n",
    "    \n",
    "    if res.returncode != 0:\n",
    "        print(f\"Error when running Nougat on {file_path}.\")\n",
    "        return res.returncode\n",
    "    else:\n",
    "        print(f\"Operation completed for {file_path}!\")\n",
    "        return 0\n",
    "\n",
    "def june_get_tables_from_mmd(mmd_path):\n",
    "    \"\"\"\n",
    "    Extract tables from an MMD file generated by Nougat.\n",
    "\n",
    "    Args:\n",
    "        mmd_path (str): Path to the MMD file.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tables extracted from the MMD file.\n",
    "    \"\"\"\n",
    "    with open(mmd_path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    res = []\n",
    "    tmp = []\n",
    "    flag = \"\"\n",
    "    \n",
    "    for line in lines:\n",
    "        if line == \"\\\\begin{table}\\n\":\n",
    "            flag = \"BEGINTABLE\"\n",
    "        elif line == \"\\\\end{table}\\n\":\n",
    "            flag = \"ENDTABLE\"\n",
    "        \n",
    "        if flag == \"BEGINTABLE\":\n",
    "            tmp.append(line)\n",
    "        elif flag == \"ENDTABLE\":\n",
    "            tmp.append(line)\n",
    "            flag = \"CAPTION\"\n",
    "        elif flag == \"CAPTION\":\n",
    "            tmp.append(line)\n",
    "            flag = \"MARKDOWN\"\n",
    "            res.append(''.join(tmp))\n",
    "            tmp = []\n",
    "    \n",
    "    return res\n",
    "\n",
    "def process_pdf(file_path, output_dir):\n",
    "    \"\"\"\n",
    "    Process a PDF file by extracting text and tables, and run Nougat tool.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input PDF file.\n",
    "        output_dir (str): Directory to store the output.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (texts_with_metadata, tables_with_metadata, pdf_id, elapsed_time)\n",
    "            - texts_with_metadata: List of documents containing the extracted text.\n",
    "            - tables_with_metadata: List of documents containing the extracted tables.\n",
    "            - pdf_id: Unique identifier for the PDF file.\n",
    "            - elapsed_time: Time taken to process the PDF.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    pdf_id = str(uuid.uuid4())\n",
    "    texts_with_metadata = []\n",
    "    tables_with_metadata = []\n",
    "\n",
    "    # Load PDF and extract text\n",
    "    reader = PdfReader(file_path)\n",
    "    pages = []\n",
    "    \n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            pages.append(Document(page_content=page_text, metadata={\"pdf_id\": pdf_id}))\n",
    "\n",
    "    # Split text using NLTKTextSplitter\n",
    "    text_splitter = NLTKTextSplitter()\n",
    "    texts = text_splitter.split_documents(pages)\n",
    "\n",
    "    # Add PDF ID to each text document's metadata\n",
    "    texts_with_metadata.extend([Document(page_content=text.page_content, metadata={\"pdf_id\": pdf_id}) for text in texts])\n",
    "\n",
    "    # Run Nougat tool\n",
    "    if june_run_nougat(file_path, output_dir) == 1:\n",
    "        print(f\"Failed to process {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract tables from MMD file generated by Nougat\n",
    "    mmd_path = os.path.join(output_dir, os.path.splitext(os.path.basename(file_path))[0] + \".mmd\")\n",
    "    tables = june_get_tables_from_mmd(mmd_path)\n",
    "    tables_with_metadata.extend([Document(page_content=table, metadata={\"pdf_id\": pdf_id}) for table in tables])\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Processed {file_path} in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    return texts_with_metadata, tables_with_metadata, pdf_id, elapsed_time\n",
    "\n",
    "def process_pdfs_in_batches(input_dir, output_dir, batch_size=10, max_workers=4):\n",
    "    \"\"\"\n",
    "    Process multiple PDFs in batches with parallel execution.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Directory containing PDF files.\n",
    "        output_dir (str): Directory to store the output.\n",
    "        batch_size (int): Number of PDFs to process in each batch.\n",
    "        max_workers (int): Maximum number of concurrent workers.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (all_texts, all_tables, pdf_ids, processing_times)\n",
    "            - all_texts: List of all extracted texts from PDFs.\n",
    "            - all_tables: List of all extracted tables from PDFs.\n",
    "            - pdf_ids: List of unique PDF IDs.\n",
    "            - processing_times: List of processing times for each PDF.\n",
    "    \"\"\"\n",
    "    pdf_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.pdf')]\n",
    "    \n",
    "    all_texts = []\n",
    "    all_tables = []\n",
    "    pdf_ids = []\n",
    "    processing_times = []\n",
    "\n",
    "    for i in range(0, len(pdf_files), batch_size):\n",
    "        batch_files = pdf_files[i:i + batch_size]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(process_pdf, pdf_file, output_dir) for pdf_file in batch_files]\n",
    "\n",
    "            # Show progress using tqdm\n",
    "            for future in tqdm(as_completed(futures), total=len(batch_files), desc=f\"Processing batch {i // batch_size + 1}\"):\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    texts, tables, pdf_id, elapsed_time = result\n",
    "                    all_texts.extend(texts)\n",
    "                    all_tables.extend(tables)\n",
    "                    pdf_ids.append(pdf_id)\n",
    "                    processing_times.append(elapsed_time)\n",
    "    \n",
    "    return all_texts, all_tables, pdf_ids, processing_times\n",
    "\n",
    "# Example usage\n",
    "input_dir = \"/path/to/pdf/directory\"\n",
    "output_dir = \"/path/to/output/directory\"\n",
    "\n",
    "# Process PDFs in batches\n",
    "texts, tables, pdf_ids, processing_times = process_pdfs_in_batches(input_dir, output_dir)\n",
    "\n",
    "# Print processing times for each file\n",
    "for pdf_file, processing_time in zip(os.listdir(input_dir), processing_times):\n",
    "    print(f\"{pdf_file} processed in {processing_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31728c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the table summarization prompt\n",
    "table_prompt_text = \"\"\"You are an assistant tasked with summarizing tables. \\\n",
    "Give a concise summary of the table by forming logical and corresponding relationships rather than broad summaries. Table chunk: {element}\"\"\"\n",
    "table_prompt = ChatPromptTemplate.from_template(table_prompt_text)\n",
    "\n",
    "# Define the text summarization prompt\n",
    "text_prompt_text = \"\"\"You are an assistant tasked with summarizing text. \\\n",
    "Give a concise summary of the text chunk. Text chunk: {element}\"\"\"\n",
    "text_prompt = ChatPromptTemplate.from_template(text_prompt_text)\n",
    "\n",
    "# Create the model instance\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "\n",
    "# Table summarization chain\n",
    "table_summarize_chain = {\"element\": lambda x: x} | table_prompt | model | StrOutputParser()\n",
    "\n",
    "# Text summarization chain\n",
    "text_summarize_chain = {\"element\": lambda x: x} | text_prompt | model | StrOutputParser()\n",
    "\n",
    "def summarize_tables(tables, max_concurrency=5):\n",
    "    \"\"\"\n",
    "    Process the tables and generate summaries.\n",
    "    \n",
    "    Args:\n",
    "        tables (list): A list of tables, each containing a text chunk.\n",
    "        max_concurrency (int): The maximum number of concurrent requests.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of table summaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table_summaries = table_summarize_chain.batch([table['content'] for table in tables], {\"max_concurrency\": max_concurrency})\n",
    "        return table_summaries\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing tables: {e}\")\n",
    "        return []\n",
    "\n",
    "def summarize_texts(texts, max_concurrency=5):\n",
    "    \"\"\"\n",
    "    Process the texts and generate summaries.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): A list of text chunks.\n",
    "        max_concurrency (int): The maximum number of concurrent requests.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of text summaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text_summaries = text_summarize_chain.batch([text['content'] for text in texts], {\"max_concurrency\": max_concurrency})\n",
    "        return text_summaries\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing texts: {e}\")\n",
    "        return []\n",
    "\n",
    "# Assuming `tables` and `texts` are already obtained\n",
    "# Get table summaries\n",
    "table_summaries = summarize_tables(tables, max_concurrency=5)\n",
    "\n",
    "# Get text summaries\n",
    "text_summaries = summarize_texts(texts, max_concurrency=5)\n",
    "\n",
    "# Print the summaries\n",
    "for i, table_summary in enumerate(table_summaries):\n",
    "    print(f\"Table {i+1} Summary: {table_summary}\")\n",
    "\n",
    "for i, text_summary in enumerate(text_summaries):\n",
    "    print(f\"Text {i+1} Summary: {text_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set persistent directory\n",
    "persist_directory = ''  # Specify your persistent directory here\n",
    "\n",
    "# Initialize the Chroma vector store with OpenAI embeddings\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings(), persist_directory=persist_directory)\n",
    "vectorstore.persist()\n",
    "\n",
    "# Create an in-memory document store\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Create the MultiVectorRetriever for handling vector and document retrieval\n",
    "MultiVector_retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    "    search_kwargs={\"k\": 10}  # Adjust the number of results to return from the search\n",
    ")\n",
    "\n",
    "# Function to add documents (text or tables) with summaries to the retriever\n",
    "def add_documents_to_retriever(contents, summaries, documents_type='text'):\n",
    "    \"\"\"\n",
    "    Adds text or table documents along with summaries to the vector store and document store.\n",
    "\n",
    "    Args:\n",
    "        contents (list): A list of documents (text or table).\n",
    "        summaries (list): A list of summaries corresponding to the documents.\n",
    "        documents_type (str): The type of document being added ('text' or 'table').\n",
    "    \"\"\"\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in contents]\n",
    "    \n",
    "    # Create the documents with content and metadata\n",
    "    documents = [\n",
    "        Document(page_content=content[\"content\"], metadata={\n",
    "            id_key: doc_ids[i],\n",
    "            \"pdf_id\": contents[i][\"metadata\"][\"pdf_id\"],\n",
    "            \"summary\": summaries[i]\n",
    "        })\n",
    "        for i, content in enumerate(contents)\n",
    "    ]\n",
    "    \n",
    "    # Add documents to the vector store and docstore\n",
    "    MultiVector_retriever.vectorstore.add_documents(documents)\n",
    "    MultiVector_retriever.docstore.mset(list(zip(doc_ids, [content[\"content\"] for content in contents])))\n",
    "\n",
    "# Add text and table documents to the retriever\n",
    "add_documents_to_retriever(texts, text_summaries, documents_type='text')\n",
    "add_documents_to_retriever(tables, table_summaries, documents_type='table')\n",
    "\n",
    "# Set the search type to MMR (Maximum Margin Retrieval)\n",
    "MultiVector_retriever.search_type = SearchType.mmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d03a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique IDs for each text and table\n",
    "text_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "\n",
    "# Initialize mappings for content and summaries\n",
    "id_to_content = {}\n",
    "id_to_summary = {}\n",
    "\n",
    "# Build the mapping for texts\n",
    "for i, text in enumerate(texts):\n",
    "    text_id = text_ids[i]\n",
    "    id_to_content[text_id] = text['content']\n",
    "    id_to_summary[text_id] = text_summaries[i]\n",
    "\n",
    "# Build the mapping for tables\n",
    "for i, table in enumerate(tables):\n",
    "    table_id = table_ids[i]\n",
    "    id_to_content[table_id] = table['content']\n",
    "    id_to_summary[table_id] = table_summaries[i]\n",
    "\n",
    "# Combine original content and summaries for BM25 input\n",
    "combined_texts_with_ids = []\n",
    "for content_id in id_to_content.keys():\n",
    "    combined_texts_with_ids.append((content_id, id_to_content[content_id]))  # Original content\n",
    "    combined_texts_with_ids.append((content_id, id_to_summary[content_id]))  # Corresponding summary\n",
    "\n",
    "# Initialize BM25Retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    [text for _, text in combined_texts_with_ids], k=10  # Top 10 results\n",
    ")\n",
    "\n",
    "# Initialize the EnsembleRetriever with BM25 and MultiVector retrievers\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, MultiVector_retriever], weights=[0.3, 1]\n",
    ")\n",
    "\n",
    "# Function to handle retrieval and ranking using EnsembleRetriever\n",
    "def retrieve_with_ensemble(query, k=10):\n",
    "    \"\"\"\n",
    "    Retrieves documents using the ensemble of retrievers (BM25 and MultiVector).\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to use for retrieval.\n",
    "        k (int): The number of results to return.\n",
    "\n",
    "    Returns:\n",
    "        List of documents retrieved based on the ensemble search.\n",
    "    \"\"\"\n",
    "    return ensemble_retriever.retrieve(query, k=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1cc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents(all_queries):\n",
    "    # Assuming ensemble_retriever is defined and accessible\n",
    "    retrieved_documents = []\n",
    "    for query in all_queries:\n",
    "        results = ensemble_retriever.get_relevant_documents(query)\n",
    "        docString = [doc.page_content for doc in results]  # Assuming each result has a 'page_content' attribute\n",
    "        retrieved_documents.extend(docString)\n",
    "    \n",
    "    # Use set for efficient deduplication\n",
    "    unique_documents = list(set(retrieved_documents))\n",
    "    \n",
    "    # Assuming the initial query is to be compared against each unique document\n",
    "    # Note: Adjust this part if you intend to use a different approach\n",
    "    pairs = [[query, doc] for doc in unique_documents]\n",
    "    \n",
    "    model = RerankerModel(model_name_or_path=\"maidalun1020/bce-reranker-base_v1\")\n",
    "    # Assuming compute_score method exists and works as expected\n",
    "    scores = model.compute_score(pairs)\n",
    "    \n",
    "    # Combine scores with documents\n",
    "    scored_documents = [{\"score\": score, \"document\": doc} for score, doc in zip(scores, unique_documents)]\n",
    "    \n",
    "    # Sort by score in descending order and select top 5\n",
    "    sorted_docs = sorted(scored_documents, key=lambda x: x['score'], reverse=True)[:20]\n",
    "    \n",
    "    # Assuming rerank method exists, adjust according to its actual usage if different\n",
    "    # This step seems redundant if we've already sorted documents based on scores\n",
    "    # You might skip reranking if compute_score already provides the relevance order\n",
    "    # Uncomment the next line if reranking with a different method/model is indeed needed\n",
    "    # final_reranked = model.rerank(...)\n",
    "\n",
    "    return sorted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e035b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents(all_queries, preserve_metadata=False):\n",
    "    \"\"\"\n",
    "    Retrieves relevant documents for a list of queries using the ensemble retriever,\n",
    "    and returns a list of unique document contents. Optionally preserves metadata.\n",
    "\n",
    "    Args:\n",
    "        all_queries (list of str): The list of queries to retrieve documents for.\n",
    "        preserve_metadata (bool): Whether to keep metadata associated with the documents.\n",
    "\n",
    "    Returns:\n",
    "        list of str or list of dict: If preserve_metadata is False, returns a list of unique document contents.\n",
    "                                     If True, returns a list of dictionaries with 'content' and 'metadata'.\n",
    "    \"\"\"\n",
    "    retrieved_documents = []\n",
    "    \n",
    "    for query in all_queries:\n",
    "        try:\n",
    "            # Retrieve relevant documents using the ensemble retriever\n",
    "            results = ensemble_retriever.get_relevant_documents(query)\n",
    "            \n",
    "            if results:\n",
    "                if preserve_metadata:\n",
    "                    # Store both the content and metadata\n",
    "                    retrieved_documents.extend([{'content': doc.page_content, 'metadata': doc.metadata} for doc in results])\n",
    "                else:\n",
    "                    # Only store the content\n",
    "                    retrieved_documents.extend([doc.page_content for doc in results])\n",
    "            else:\n",
    "                print(f\"No results found for query: {query}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving documents for query '{query}': {e}\")\n",
    "    \n",
    "    # Use set for efficient deduplication (converting to set will remove duplicates)\n",
    "    unique_documents = list({doc['content'] if preserve_metadata else doc for doc in retrieved_documents})\n",
    "    \n",
    "    return unique_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9591776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(temperature = 0, model = \"gpt-4\")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": RunnableLambda(create_original_query)| RunnableLambda(create_documents), \"question\": RunnablePassthrough()} \n",
    "    | prompt  \n",
    "    | model  \n",
    "    | StrOutputParser()  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e45ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "file_path = ''\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Extract questions and answers\n",
    "questions = df['user_input'].tolist()\n",
    "ground_truth = df['reference'].tolist()\n",
    "\n",
    "# Initialize empty lists for answers and contexts\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 5\n",
    "\n",
    "# Retry logic for SSL errors\n",
    "def retry_request(func, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "                delay *= 2  # Exponential backoff\n",
    "            else:\n",
    "                print(f\"Max retries reached: {e}\")\n",
    "                return None\n",
    "\n",
    "# Generate answers and contexts in batches to reduce load and avoid SSL errors\n",
    "for i in range(0, len(questions), batch_size):\n",
    "    batch_questions = questions[i:i + batch_size]\n",
    "    batch_answers = []\n",
    "    batch_contexts = []\n",
    "\n",
    "    for query in batch_questions:\n",
    "        # Adding delay to avoid overwhelming the server\n",
    "        time.sleep(2)  # Shorter delay since we have retry logic\n",
    "\n",
    "        # Using ensemble_retriever to get context (Replace with your retriever logic)\n",
    "        relevant_documents = retry_request(lambda: ensemble_retriever.get_relevant_documents(query))\n",
    "        if relevant_documents is None:\n",
    "            batch_contexts.append([])\n",
    "            batch_answers.append(\"No relevant context found due to an error.\")\n",
    "            print(f\"Error fetching documents for query: {query}\")\n",
    "            continue\n",
    "\n",
    "        document_contents = [doc.page_content for doc in relevant_documents]\n",
    "        batch_contexts.append(document_contents)\n",
    "\n",
    "        # Using context to generate an answer (Replace with your model invocation logic)\n",
    "        if document_contents:  # Check if there is context content\n",
    "            context_string = \"\\n\".join(document_contents)\n",
    "            answer = retry_request(lambda: chain.invoke({\"context\": context_string, \"question\": query}))\n",
    "            if answer is None:\n",
    "                answer = \"Error during model invocation.\"\n",
    "        else:\n",
    "            answer = \"No relevant context found.\"\n",
    "        batch_answers.append(answer)\n",
    "\n",
    "    # Append current batch results to the main lists\n",
    "    answers.extend(batch_answers)\n",
    "    contexts.extend(batch_contexts)\n",
    "\n",
    "# Construct the dataset\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truth,\n",
    "}\n",
    "\n",
    "# Create the dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Optionally, save dataset to disk for later use\n",
    "dataset.save_to_disk(\"processed_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d862bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms.base import LangchainLLMWrapper\n",
    "from langchain_community.chat_models import ChatOpenAI  # Modify import\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    RubricsScoreWithReference,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "# Create Langchain LLM instance, e.g., ChatOpenAI\n",
    "langchain_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Wrap the Langchain LLM instance in LangchainLLMWrapper\n",
    "wrapped_llm = LangchainLLMWrapper(langchain_llm)\n",
    "\n",
    "# Define custom rubrics\n",
    "my_custom_rubrics = {\n",
    "    \"score1_description\": \"The response is incorrect, irrelevant, or does not align with the ground truth.\",\n",
    "    \"score2_description\": \"The response partially matches the ground truth but includes significant errors, omissions, or irrelevant information.\",\n",
    "    \"score3_description\": \"The response generally aligns with the ground truth but may lack detail, clarity, or have minor inaccuracies.\",\n",
    "    \"score4_description\": \"The response is mostly accurate and aligns well with the ground truth, with only minor issues or missing details.\",\n",
    "    \"score5_description\": \"The response is fully accurate, aligns completely with the ground truth, and is clear and detailed.\",\n",
    "}\n",
    "\n",
    "# Use the dataset you previously constructed\n",
    "# dataset = Dataset.from_dict(data)  # Constructed earlier with questions, answers, and ground truth\n",
    "\n",
    "# Instantiate the metric class\n",
    "metric_with_ref = RubricsScoreWithReference(rubrics=my_custom_rubrics)\n",
    "\n",
    "# Perform the evaluation on the dataset\n",
    "try:\n",
    "    result = evaluate(\n",
    "        dataset=dataset,\n",
    "        metrics=[\n",
    "            metric_with_ref,\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "            answer_correctness,\n",
    "            answer_similarity\n",
    "        ],\n",
    "        llm=wrapped_llm  # Use the wrapped LLM for evaluation\n",
    "    )\n",
    "    # Print the evaluation results\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatrag",
   "language": "python",
   "name": "chatrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
